{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyuBZii1PGuuwCeDUvFPUJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SOUBIMvdDVC7","outputId":"40e04499-22af-487a-9b46-664a5863d95e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n"," Scraping papers for year 2021...\n"," Found 1849 papers for 2021\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","import random\n","import re\n","from google.colab import drive  # Google Drive integration\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# File path to save in Google Drive\n","SAVE_PATH = \"/content/drive/My Drive/Minor/papers_2021.csv\"\n","\n","# ACL Anthology event URL format\n","EVENT_URL_TEMPLATE = \"https://aclanthology.org/events/acl-{year}/\"\n","\n","# Years for which we want to scrape papers\n","START_YEAR = 2021\n","END_YEAR = 2021\n","\n","# Function to clean extracted BibTeX values\n","def clean_text(text):\n","    \"\"\"Removes extra spaces, LaTeX artifacts, and special characters.\"\"\"\n","    text = text.replace(\"{\", \"\").replace(\"}\", \"\")\n","    text = text.replace(\"\\\\ensuremath{>}\", \">\")  # Fix LaTeX math symbols\n","    text = text.replace(\"\\\\url\", \"\")  # Remove \\url references\n","    text = text.replace(\"{--}\", \"â€“\")  # Convert LaTeX dashes\n","    text = text.strip()  # Remove extra spaces\n","    return text\n","\n","# Function to parse BibTeX for a single paper\n","def parse_bibtex(bibtex_text, paper_id):\n","    \"\"\"\n","    Parses a BibTeX formatted string and extracts volume name, title, authors, abstract, and URL.\n","    Handles multi-line authors and cleans extracted text.\n","    \"\"\"\n","    title_match = re.search(r'title\\s*=\\s*[{\"](.+?)[}\"]', bibtex_text, re.DOTALL)\n","    author_match = re.findall(r'author\\s*=\\s*\"(.*?)\"', bibtex_text, re.DOTALL)\n","    year_match = re.search(r'year\\s*=\\s*[{\"](\\d+)[}\"]', bibtex_text)\n","    url_match = re.search(r'url\\s*=\\s*[{\"](.+?)[}\"]', bibtex_text)\n","    booktitle_match = re.search(r'booktitle\\s*=\\s*[{\"](.+?)[}\"]', bibtex_text, re.DOTALL)\n","\n","    title = clean_text(title_match.group(1)) if title_match else \"Title Not Found\"\n","\n","    # Fix multi-line author parsing\n","    authors = []\n","    for match in author_match:\n","        authors.extend([a.strip() for a in match.split(\"  and\\n \")])  # Handle 'and' separators\n","    authors = \", \".join(authors) if authors else \"Authors Not Found\"\n","\n","    year = year_match.group(1) if year_match else \"Year Not Found\"\n","    paper_url = f\"https://aclanthology.org/{paper_id}\"\n","    pdf_link = f\"https://aclanthology.org/{paper_id}.pdf\"  # Direct PDF link\n","    volume_name = clean_text(booktitle_match.group(1)) if booktitle_match else \"Volume Name Not Found\"\n","\n","    return {\n","        \"volume_name\": volume_name,\n","        \"title\": title,\n","        \"authors\": authors,\n","        \"year\": year,\n","        \"pdf_link\": pdf_link,\n","        \"url\": paper_url\n","    }\n","\n","# Function to scrape a single paper's BibTeX page\n","def scrape_paper(paper_id):\n","    paper_bib_url = f\"https://aclanthology.org/{paper_id}.bib\"\n","    response = requests.get(paper_bib_url)\n","\n","    if response.status_code != 200:\n","        return None  # Skip failed requests\n","\n","    bibtex_text = response.text.strip()  # Get the BibTeX content\n","    return parse_bibtex(bibtex_text, paper_id)  # Extract useful details\n","\n","# Function to scrape all papers from a given year\n","def scrape_year(year):\n","    event_url = EVENT_URL_TEMPLATE.format(year=year)\n","    response = requests.get(event_url)\n","\n","    if response.status_code != 200:\n","        print(f\"Failed to retrieve ACL event page for {year}\")\n","        return []\n","\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    # Extract proper paper identifiers (without slashes)\n","    paper_ids = [\n","        a['href'].strip(\"/\")\n","        for a in soup.find_all('a', href=True)\n","        if a['href'].startswith(f\"/{year}.\") and not a['href'].endswith(\".bib\")\n","    ]\n","\n","    if not paper_ids:\n","        print(f\" Found 0 papers for {year}\")\n","        return []\n","\n","    print(f\" Found {len(paper_ids)} papers for {year}\")\n","\n","    papers_data = []\n","    for paper_id in paper_ids:\n","        paper_data = scrape_paper(paper_id)\n","        if paper_data:\n","            papers_data.append(paper_data)\n","\n","        # Add random delay to prevent rate limiting\n","        time.sleep(random.uniform(1, 3))\n","\n","    return papers_data\n","\n","# Function to scrape multiple years and save to CSV\n","def scrape_acl_papers():\n","    all_papers = []\n","\n","    for year in range(START_YEAR, END_YEAR + 1):\n","        print(f\"\\n Scraping papers for year {year}...\")\n","        papers = scrape_year(year)\n","        all_papers.extend(papers)\n","        print(f\" {len(papers)} papers scraped for {year}.\")\n","\n","    # Convert data to DataFrame\n","    df = pd.DataFrame(all_papers)\n","\n","    # Save to Google Drive without displaying contents\n","    df.to_csv(SAVE_PATH, index=False)\n","    print(f\"\\n Scraping complete! Data saved to {SAVE_PATH}\")\n","\n","# Run the scraper\n","scrape_acl_papers()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"M2tRcywdDX0j"},"execution_count":null,"outputs":[]}]}